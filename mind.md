# 观点

这是我对于深度学习的一些看法，有的仅仅是一条简单的笔记，大部分是自己的独立观点。

深度学习本质上并不是去拟合实际系统，而是做特征学习。

深度学习是端到端学习，自动学习特征和映射，不用像以前一样进行手工的特征设计。但目前的发展程度还很低，人工设计网络结构一定程度上还是属于特征设计，还是需要通过先验信息来限定特征结构、模型容量。

深度学习的过程也是一场机器认知的过程。

深度学习的方法都显得非常技巧，一个根本原因是因为深度学习研究的是高度非线性的系统，所以往往无法通过个别高度概括性的工具应对各种问题。所以研究深度学习需要理论结合实际，不能纸上谈兵。

学到的参数就是在表征一些模式，参数共享也是促使模型学习主要模式，同时也可以起到抑制过拟合的作用，因为有限的参数要尽量满足主要特征，就自然忽略了噪声。

神经网络不同的层结构（全连接、卷积…）是不同的特征学习器，它们有着不同的先验假设，适合学习不同的特征。

大脑不会做反向传播的，但是一定也有相应的反馈机制。

各种类型的网络层与层之间都不应该缩减太快，因为后面的后面神经元学习的特征很难有效利用前面神经元的众多特征。

卷积核的大小也会影响卷积核的表达能力，参数量小的卷积核所能产生的可能性也少。
原则上说MLP就可以拟合各种情况的模型，之所以需要设计、发展各种结构，一方面是直接用MLP会参数爆炸；
另一方面是在平衡困难：直接用MLP的话是把困难都推给了优化过程，因为对模型不加限制会使得很难训练到想要的参数，而设计各种结构是通过加入先验信息来限制模型能力，使得模型往想要的方向前进。

任何神经网络结构都可以看成全连接层加上不同程度、不同角度的先验。
例如：卷积层也可以看做是一个全连接层，但是它的权值有两个无限强的先验：层内的权重都是重复的，同一个权重将复用多次；层输出单元的接受区域内的权重非零，其它权重都是零。

大的梯度容易跳出局部极值，但是只能学到简单的pattern，小的梯度不容易跳出局部极值，但是有利于学到复杂的pattern。
在训练过程中，学习率一般是要逐步减小的，但是在陷入局部极值的时候增大学习率或许可以帮助跳出。

一般解释sigmoid函数在深度学习上的劣势通常是从梯度消失的角度来解释，也可以尝试另一个角度：sigmoid函数就不是为深度学习而生的，它原本就是在浅层模型（逻辑斯蒂回归）中将数值从无穷区间映射到0~1之间的工具。现在这一工作基本由softmax替代，所以替代sigmoid函数的不是relu，而是softmax。另外softmax本身就是对sigmoid的扩展。

Relu函数虽然有一半的输出处于0导数，但却并不是说会有一半的神经元处于未激活的状态，因为对于不同的样本可能函数输入会很不相同，有的样本输入大于0，有的样本输入小于0，这样就不能算未激活。

Tanh几乎在所有场合都优于sigmoid函数，除非要求输出0~1，否则需要考虑sigmoid的时候完全可以用tanh取代。

每一个神经元都是一个特征学习器，因此怎样的激活函数更好也可以从这个角度展开分析。所以前馈网络的隐单元更适合relu，而tanh（sigmoid）用在需要限制输出的地方，如循环神经网络中防止梯度爆炸。

反向传播是对多元微分的一种实现。利用动态规划的思想，用空间换时间，存储中间结果，避免链式法则中的大量重复计算。

为什么一般选择存储残差而不是其它节点，可以从图的角度解释，残差位置特殊，是多路信息的交汇处，一方面可以直接从残差推导出邻近参数梯度，另一方面其梯度（残差）需要继续向后传递。

直连有利于梯度传递到底层因而便于训练，从计算图上看，权重衰减的正则化也是一种直连，但是并没有通过权重衰减建立与标签信息的联系，只是建立与一些先验信息的联系。

平时常说过拟合是因为模型容量大，而正则化抑制过拟合是限制了模型容量，这样讲基本正确，但是传递一种意思好像模型容量大是原罪一样。实际上容量大应该是优势，一匹好马多少会有点倔劲，我们要把模型容量和不合适的模型空间区分开，不能把训练、优化的问题推给模型。

Dropout是bagging的一种特殊实现，各个子模型之间会共享参数以降低训练的参数复杂度。

如果一个训练集可以唯一确定一组模型参数，则该模型称作可辨认的。
带有隐变量的模型通常是不可辨认的。因为可以批量交换隐变量，从而得到等价的模型。如：交换隐单元和的权重向量。
这种不可辨认性称作权重空间对称性weight space symmetry 。
也可以放大权重和偏置α倍，然后缩小输出1/α倍，从而保持模型等价。
模型可辨认性问题意味着：
神经网络的代价函数具有非常多、甚至是无限多的局部极小解。
由可辨认性问题产生的局部极小解都具有相同的代价函数值，它并不是代价函数非凸性带来的问题。
假如存在一组参数使得模型、数据达到全局最小，可以通过一些变换得到无数组全局最优参数。

目前很多观点将神经网络优化中的所有困难都归结于局部极小值。
有一种方案是排除局部极小值导致的困难（说明是其他原因导致的困难）：
绘制梯度范数随着时间的变化：
如果梯度范数没有缩小到一个很小的值，则问题的原因既不是局部极小值引起的，也不是其他形式的临界点（比如鞍点）引起的。
如果梯度范数缩小到一个很小的值，则问题的原因可能是局部极小值引起的，也可能是其他原因引起的。

在极高维的时候，鞍点出现的概率要远远大于极小点。

当位于函数值较低的区间时，黑塞矩阵的特征值为正的可能性更大。这意味着：
具有较大函数值的临界点更可能是鞍点，因为此时黑塞矩阵的特征值可能既存在正值、也存在负值。
具有较小函数值的临界点更可能是局部极小值点，因为此时黑塞矩阵的特征值更可能全部为正值。
具有极高函数值的临界点更可能是局部极大值点，因为此时黑塞矩阵的特征值更可能全部为负值。

使用小批量样本来估计梯度的原因：
使用更多样本来估计梯度的方法的收益是低于线性的。
独立同分布的样本的均值是个随机变量。
如果能够快速计算出梯度的估计值（而不是费时地计算准确值），则大多数优化算法会更快收敛。
大多数优化算法基于梯度下降，如果每一步中计算梯度的时间大大缩短，则它们会更快收敛。
训练集存在冗余。
实践中可能发现：大量样本都对梯度做出了非常相似的贡献。
最坏情况下，训练集中的所有样本都是相同的拷贝。此时基于小批量样本估计梯度的策略也能够计算正确的梯度，同时节省了大量时间。

使用海森矩阵的优化算法需要更大的batch-size，因为海森矩阵的条件数过高，对于偏差的容忍度差。

mini-batch随机梯度下降中，只要没有重复使用样本，它就是真实泛化误差梯度的无偏估计。

第k步的学习率记做ϵ_k，则对于学习率，保证SGD收敛的一个充分条件是：
∑_(k=1)^∞▒ϵ_k =∞，且∑_(k=1)^∞▒〖ϵ_k〗^2 =0。

基于梯度的优化算法中，每个参数都有一个最佳的学习率范围，但实际中不可能给每个参数都设置一个学习率超参数，而一个学习率又无法满足所有的参数。这个时候就有了很多的优化角度：
对数据做归一化操作、加批次归一化层以改善参数的一致性；
自适应调整学习率；
动量法自适应调整梯度（动量）等。

在随机梯度下降中，学习率固定，因此随着梯度大小变化，参数更新量可能为任意数。而RMSProp动量法通过自适应的方式将参数更新量从无穷区间映射到了大约[-ϵ,ϵ]（定性得到的大约区间，不一定）。

梯度下降法不能保证代价函数一定下降，牛顿法更不能。
梯度下降法利用一阶泰勒展开，下降要求在小领域内；
牛顿法利用二阶泰勒展开，试图直接跳到极值点，在凸二次函数下可以一步达到最优，在其它凸优化情况下也可以较快达到最优，但是在非凸问题时没有保证，它的目标是尽快把每个参数都送到极值点，因此在多参数的时候大概率会跳到鞍点处，因为鞍点出现的概率要远远大于极大值点和极小值点。
所以牛顿法不适合非凸优化。

牛顿法要应用在非凸优化时需要正则化使得海森矩阵正定，这只适合负特征值绝对值较小时，另外海森矩阵的边长等于参数量，所以在参数量极大的深度学习应用中，牛顿法的巨大计算和存储代价也是致命缺点。

深度网络难以训练有许多原因，除了容易梯度爆炸和梯度消失外，还包括参数的依赖，高层参数靠近输出相对容易学习但是高层参数又依赖底层参数，底层参数优化不好，高层参数也无法优化，或者说高层参数的学习几乎无效。

理解BN层的一个角度：
原来的网络就像一大段代码叠在一起，可读性差（层与层之间相互依赖，难以训练）；
加上BN层之后就像把以前的代码分割成了一个个子程序，依此调用，程序之间有清晰的接口，因此阅读和调试都更加容易；
这就像是一种解耦操作，让各个层专注自己的任务（特征学习）。
同时也是在改善参数的一致性。

学习应该从简单的问题学起，这样更容易步入正轨，要是让幼儿园知识储备的神童直接学大学知识，也许会管用，但是基本知识的欠缺可能会在某一天促成大错。

boosting的做法也可以看成是拟合残差：
已有的模型已经可以工作，后来的模型主要拟合前面分类错误的样本（部分）。

循环网络在于捕捉传递（转换）关系。

卷积网络后面常常将特征图拍平，使用全连接层得到输出，这样无法适应图片尺寸变化的情况。可以用卷积层来取代全连接层，不仅可以适应图片尺寸的变化，还可以增加效率。

卷积网络中可以通过多个小卷积核来代替大卷积核，感受野不变、参数量下降、同时模型抽象能力提升，但是训练难度也上升；
也可以用两个非对称卷积核（1*n+n*1）来代替大卷积核，感受野不变、参数量下降，比较适合小特征图而不适合大特征图。
将不同大小卷积核组合使用可以同时挖掘不同尺寸特征。
1\*1卷积不改变特征图大小，但可以改变通道数，同时增加非线性。
DepthWise：单通道卷积+1\*1卷积，极大降低参数量

空洞卷积相当于一个大卷积核将部分参数固定为0，在不提高参数量的情况下扩大感受野。但是网格效应可能导致许多输入像素没有得到使用，可以使用混合空洞卷积来避免网格效应，需要保证空洞率没有大于1的公约数。

尽管神经网络有很强的特征学习、端到端学习能力，但是如果能够先验的知道一些特征对于任务十分重要，那么人工设计这些特征会更好。

卷积核的数量对应可以学到特征数量的上限。

RNN的记忆长度太短，隐单元以指数滑动方式更新，就像看无脑娱乐视频；
LSTM、GRU记忆长度虽然短，但是门结构控制记什么，就像记电话号码、地址一样；
Attention是反复的泛读全篇来提取相关信息，就像缩写文章。

attention既可以处理序列信息，又可以并行。

由单层encoder和单层decoder组成的auto-encoder与PCA对应，通过矩阵将向量映射到低维空间再升维。只是PCA强制选取方差（奇异值）大的方向来构成权重矩阵，auto-encoder则通过学习得到。
所以auto-encoder的encoder和decoder可以考虑共享参数，但这也并不必要。
同时PCA相当于是最基本的线性auto-encoder，通过更加复杂的非线性auto-encoder可以得到更好地降维效果。

auto-encoder曾经常用于深度神经网络的预训练，现在有许多技巧来更好地训练深度网络，但是在标签样本少无标签样本多的情况下，auto-encoder还是可以用在使用大量的无标签样本来预训练网络使得网络的效果更好。

去噪自动编码器通过加入噪声，使模型学习分辨并去除噪声，可以增加模型的鲁棒性。

auto-encoder不仅可以用全连接层，同样可以采用卷积层等各种结构来适应不同任务。

反卷积本质上还是卷积运算。

生成模型的一个难点，难以评估效果。

一些生成模型方法：
PixelRNN：将像素看成序列，依次生成，无法捕捉空间信息，也不能真正学习生成数据分布；
（变分）自编码器：基本的自编码器也有这个潜力，但是由于模型的非线性，使得对decoder输入随机值时得不到有意义的结果，变分通过方差扩大了中间层的范围。
GAN

词表示学习阶段：
独热码：表示相互正交，无法表达关系
词嵌入：映射到一个新的，维度更低的空间，可以表达词的关系，Word2vec

图embedding和词embedding都是通过节点（词）的相似性来提取特征，这似乎没有完全利用数据的信息，所以应该还有很大的提升空间，当然这个取决于具体的任务。

强化学习解决的问题是序列决策问题，不同于经典的分类、回归。
与其它学习算法不同，强化学习需要与环境交互，还会影响环境。

VAE更多地是在模仿，而没有怎么“主动”地去生成。

强化学习的基本过程：观察环境状态state，采取行动action，获得奖励reward，再次观察状态。

强化学习的一个难点在于奖励的延迟。

强化学习可以分为基于policy的方法、基于value的方法、基于model的方法等。

基于policy的强化学习是学习一个映射，根据observation（state）来得到action，reward用于指导学习（训练）过程。

马尔科夫过程（MP）包括状态集合和状态转移矩阵；
马尔科夫奖励过程（MRP）除了状态集合和状态转移矩阵，还包括奖励函数（计算某一个状态能够获得的奖励）和折扣系数；
马尔科夫决策过程（MDP）是带决策的马尔科夫奖励过程，包括状态集合、状态转移矩阵、决策、奖励函数、折扣系数。

监督学习要得到从x到y的映射关系，y直接拟合标签label即可；
强化学习要得到状态state到行动action的映射，但是行动action没有直接的标签可用，可以通过奖励reward间接实现标签的作用，即使得到的行动action满足reward最大化的要求间接实现对action的监督。

在训练时向模型中添加随机噪声可以增加模型的鲁棒性，通过对抗噪声可以进一步提高鲁棒性。

对抗噪声一般需要限制幅度，即修改后的p范数变化。

强化学习策略可分为确定性策略和随机性策略。
给定一个状态 s，根据确定性策略π，就可以确定需要采取的行动a，即：a = π（s）；
随机性策略给出采取不同行动的概率，即给定一个状态s，参与者采取行动a的概率为：π（a|s）= P{A_t = a|S_t = s}。

自监督学习是指在没有人工标注的数据上运行的监督学习。

许多神经网络结构如CNN往往只能在规则的欧式空间数据上使用。

虽然深度神经网络是非凸的，但是参数多，可能多，刚开始优化的时候不太可能所有参数一起陷入局部最小，即使一部分参数陷入了局部最小，还有其它参数可以优化，随着其它参数的优化，所有参数的优化曲线都会变化。
